{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a03645",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "markov decision procsses\n",
    "\n",
    "목표: MDP를 소개하고, 어떻게 파이썬에서 코딩하는지 알아보는 것\n",
    "\n",
    "MDP모델은 외부 환경과 연속적인 상호작용을 해야 함 -> 다음의 것들이 필요\n",
    "- 상태 space\n",
    "- action에 대한 집합\n",
    "- 전이 함수: t에서의 s,a가 주어질 때, t+1에서의 s'을 확률로 표현하는 것 (여기서는 물리학 법칙에 따라 결정)\n",
    "- reward함수: t에서 결정되는 reward(upright = 1, fallen over = 0)\n",
    "- 할인율: $γ$ (=1)\n",
    "\n",
    "$argmax_{\\pi} \\sum^{T}_{t=1} \\gamma R_t(\\pi)$ 를 구해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6b4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f134d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330eea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00022455 -0.00535896  0.03023214 -0.03310407]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be165f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [-0.00033173 -0.20090112  0.02957006  0.26896203]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "# action을 통해 다음 단계로 진행하기\n",
    "action = 0 \n",
    "state, reward, done, info = env.step(action)\n",
    "\n",
    "print(f\"state: {state}\")  # action을 통해 얻는 새로운 상태\n",
    "print(f\"reward: {reward}\") # 받은 보상\n",
    "print(f\"done: {done}\") # 끝났는지 여부\n",
    "print(f\"info: {info}\") # 추가 정보들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d293cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0\n",
      "18.0\n"
     ]
    }
   ],
   "source": [
    "# 무작위 작업을 수행하고 리워드 합계를 반환\n",
    "def random_rollout(env):\n",
    "    # initialize\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # 랜덤으로 행동을 선택\n",
    "        action = np.random.choice([0, 1])\n",
    "        # 환경과 상호작용하여 정보 획득\n",
    "        state, reward, done, info = env.step(action)\n",
    "        # 리워드 더하기\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    return cumulative_reward\n",
    "\n",
    "reward = random_rollout(env)\n",
    "print(reward)\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a046091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env, policy를 받아서 랜덤한 방식 말고, 정책에 따라 행동을 결정하도록 코딩하기\n",
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # ===== your code =====\n",
    "    while not done:\n",
    "        action = sample_policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "    # ===== your code =====\n",
    "\n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy(state):\n",
    "    if state[0] < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c123ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35\n"
     ]
    }
   ],
   "source": [
    "reward = np.mean([rollout_policy(env, sample_policy) for _ in range(100)])\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6470d9",
   "metadata": {},
   "source": [
    "## PPO(proximal policy optimization)\n",
    "\n",
    "1. 병렬적으로 많은 rollout들을 수행 \n",
    "2. 합친 후 SGD를 수행 (목적함수를 최대화하는 정책 찾기)\n",
    "3. 다시 rollout들에게 새로운 정책 가중치를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce012156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a2c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0219 04:09:07.638053276    4611 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0219 04:09:07.673935961    4611 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0219 04:09:07.701426697    4611 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.182.0.2',\n",
       " 'raylet_ip_address': '10.182.0.2',\n",
       " 'redis_address': '10.182.0.2:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-02-19_04-09-06_484024_4611/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-02-19_04-09-06_484024_4611/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-02-19_04-09-06_484024_4611',\n",
       " 'metrics_export_port': 63406,\n",
       " 'gcs_address': '10.182.0.2:43235',\n",
       " 'node_id': '867f023c67c1ddcd664e88740bb11b6186b4191b6a3562c736ab004c'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=3, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e3660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 04:09:09,103\tINFO trainer.py:2054 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-02-19 04:09:09,105\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-02-19 04:09:09,106\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-02-19 04:09:14,987\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"num_sgd_iter\"] = 5\n",
    "config[\"sgd_minibatch_size\"] = 256\n",
    "# 히든 레이어의 사이즈 조정\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [100, 100]\n",
    "# 셀이 재실행 될 때 리소스 부족을 막을 수 있도록 함\n",
    "config['num_cpus_per_worker'] = 0\n",
    "\n",
    "agent = PPOTrainer(config, \"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d649fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 04:09:18,964\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2022-02-19_04-09-19\n",
      "done: false\n",
      "episode_len_mean: 21.315508021390375\n",
      "episode_media: {}\n",
      "episode_reward_max: 59.0\n",
      "episode_reward_mean: 21.315508021390375\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 187\n",
      "episodes_total: 187\n",
      "experiment_id: 6887ed9c42794e0da2c54b82105b0716\n",
      "hostname: instance-1\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6914097666740417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0018208251567557454\n",
      "        model: {}\n",
      "        policy_loss: -0.015460265800356865\n",
      "        total_loss: 214.84207153320312\n",
      "        vf_explained_var: -8.742014870222192e-06\n",
      "        vf_loss: 214.85714721679688\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "  num_steps_trained_this_iter: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 10.182.0.2\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.428571428571427\n",
      "  ram_util_percent: 10.814285714285715\n",
      "pid: 4611\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07465135392711272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.07556450721532873\n",
      "  mean_inference_ms: 0.7128944339766498\n",
      "  mean_raw_obs_processing_ms: 0.10801523633373879\n",
      "time_since_restore: 4.41391134262085\n",
      "time_this_iter_s: 4.41391134262085\n",
      "time_total_s: 4.41391134262085\n",
      "timers:\n",
      "  learn_throughput: 9389.642\n",
      "  learn_time_ms: 426.001\n",
      "  load_throughput: 10472669.164\n",
      "  load_time_ms: 0.382\n",
      "  sample_throughput: 1005.651\n",
      "  sample_time_ms: 3977.524\n",
      "  update_time_ms: 2.536\n",
      "timestamp: 1645243759\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2022-02-19_04-09-23\n",
      "done: false\n",
      "episode_len_mean: 25.825806451612902\n",
      "episode_media: {}\n",
      "episode_reward_max: 86.0\n",
      "episode_reward_mean: 25.825806451612902\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 155\n",
      "episodes_total: 342\n",
      "experiment_id: 6887ed9c42794e0da2c54b82105b0716\n",
      "hostname: instance-1\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6730445623397827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005629027262330055\n",
      "        model: {}\n",
      "        policy_loss: -0.023416943848133087\n",
      "        total_loss: 286.9964599609375\n",
      "        vf_explained_var: -1.1026064385077916e-05\n",
      "        vf_loss: 287.019287109375\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "  num_steps_trained_this_iter: 4000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 10.182.0.2\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.483333333333334\n",
      "  ram_util_percent: 10.9\n",
      "pid: 4611\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07473610443050392\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0756782466896533\n",
      "  mean_inference_ms: 0.7153912017292089\n",
      "  mean_raw_obs_processing_ms: 0.10687573941524828\n",
      "time_since_restore: 8.599364757537842\n",
      "time_this_iter_s: 4.185453414916992\n",
      "time_total_s: 8.599364757537842\n",
      "timers:\n",
      "  learn_throughput: 12474.866\n",
      "  learn_time_ms: 320.645\n",
      "  load_throughput: 11436411.725\n",
      "  load_time_ms: 0.35\n",
      "  sample_throughput: 951.971\n",
      "  sample_time_ms: 4201.811\n",
      "  update_time_ms: 2.383\n",
      "timestamp: 1645243763\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# episode len mean 값을 보기\n",
    "# 최대는 200. for문을 30번정도 하면 나올듯?\n",
    "for i in range(2):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3608fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 04:09:23,639\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dojm.ex5/ray_results/PPOTrainer_CartPole-v0_2022-02-19_04-09-09vfljfcxx/checkpoint_000002/checkpoint-2\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20eb229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 04:09:29,329\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-19 04:09:29,385\tINFO trainable.py:472 -- Restored on 10.182.0.2 from checkpoint: /home/dojm.ex5/ray_results/PPOTrainer_CartPole-v0_2022-02-19_04-09-09vfljfcxx/checkpoint_000002/checkpoint-2\n",
      "2022-02-19 04:09:29,387\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': 8000, '_time_total': 8.599364757537842, '_episodes_total': 342}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOTrainer(trained_config, \"CartPole-v0\")\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99098500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 04:09:29,395\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0\n"
     ]
    }
   ],
   "source": [
    "# 불러온 데이터가 잘 작동하는지 테스트하기\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    \n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b8ab9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=~/ray_Results --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e496c",
   "metadata": {},
   "source": [
    "## custom environment & reward shaping\n",
    "\n",
    "아래 두 가지에 포커스\n",
    "\n",
    "1. 어떻게 MDP추상화를 만들 수 있을지\n",
    "2. 너의 에이전트를 더 효율적으로 만들기 위해 커스텀 환경에 따른 보상을 어떻게 설정할지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31813a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "# git clone을 해서 할 경우\n",
    "# !git clone https://github.com/ray-project/tutorial\n",
    "# from tutorial.rllib_exercises import test_exercises\n",
    "\n",
    "# 그냥 따로 저장한 경우\n",
    "import test_exercises\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090aca59",
   "metadata": {},
   "source": [
    "#### Different Spaces\n",
    "\n",
    "action, observation space의 dimension을 정하는 것은 RL을 공식화 할 때 가장 먼저 해야할 일 (-> gym에서 이러한 것들을 제공)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524604ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "discrete = spaces.Discrete(10)\n",
    "print([discrete.sample() for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b95a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "('discrete_10', 2)\n",
      "True\n",
      "('box_1', [0.1])\n",
      "True\n",
      "('box_3x1', [[0], [0], [0]])\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dojm.ex5/rl/venv/lib/python3.8/site-packages/gym/spaces/box.py:142: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    }
   ],
   "source": [
    "action_space_map = {\n",
    "    \"discrete_10\": spaces.Discrete(10),\n",
    "    \"box_1\": spaces.Box(0, 1, shape=(1,)),\n",
    "    \"box_3x1\": spaces.Box(-2, 2, shape=(3, 1))\n",
    "}\n",
    "\n",
    "action_space_jumble = {\n",
    "    \"discrete_10\": 2,\n",
    "    \"box_1\": [0.1],\n",
    "    \"box_3x1\": [[0], [0], [0]]\n",
    "}\n",
    "\n",
    "\n",
    "for space_id, state in action_space_jumble.items():\n",
    "    print(action_space_map[space_id].contains(state))\n",
    "    print((space_id, state))\n",
    "    assert action_space_map[space_id].contains(state), (\n",
    "        \"Looks like {} to {} is matched incorrectly.\".format(space_id, state))\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be275f",
   "metadata": {},
   "source": [
    "#### setting custom env with rewards\n",
    "\n",
    "n-chain 환경 에서 다음 두가지로 동작\n",
    "1. forward: chain따라 움직이지만 리워드는 0\n",
    "2. backword: 처음으로 돌아가는데 작은 리워드 반환\n",
    "    \n",
    "-> 그런데 체인의 마지막에서는 큰 리워드 발생(지속적으로 작은 보상 대신 forward를 선택해야 함)\n",
    "\n",
    "목적: 이러한 환경 구성하기\n",
    "    \n",
    "1. ChainEnv._setup_spaces\n",
    "    - observation space: 0 ~ n-1\n",
    "    - action space: 0 or 1\n",
    "2. reward function\n",
    "    - action == 1일 때 return self.small_reward\n",
    "    - action == 0이고 self.state < self.n - 1일 때 return 0\n",
    "    - action == 0이고 self.state == self.n-1일 때 return self.large_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ede4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if spaces have been setup correctly...\n",
      "Success! You've setup the spaces correctly.\n",
      "Testing if reward has been setup correctly...\n",
      "Success! You've setup the rewards correctly.\n"
     ]
    }
   ],
   "source": [
    "class ChainEnv(gym.Env):\n",
    "    def __init__(self, env_config={}):\n",
    "        env_config = env_config\n",
    "        self.n = env_config.get(\"n\", 20)\n",
    "        self.small_reward = env_config.get(\"small\", 2)\n",
    "        self.large_reward = env_config.get(\"large\", 10)\n",
    "        self.state = 0\n",
    "        self._horizon = self.n\n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "        \n",
    "    def _setup_spaces(self):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n)\n",
    "        \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        if action == 1:\n",
    "            reward = self.small_reward\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:\n",
    "            reward = 0\n",
    "            self.state += 1\n",
    "        else:\n",
    "            reward = self.large_reward\n",
    "            \n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self._counter = 0\n",
    "        return self.state\n",
    "    \n",
    "test_exercises.test_chain_env_spaces(ChainEnv)\n",
    "test_exercises.test_chain_env_reward(ChainEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4001d22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 11:58:16,452\tWARNING ppo.py:223 -- `train_batch_size` (400) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 133.\n",
      "E0219 11:58:17.608685267   11687 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0219 11:58:17.640864842   11687 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0219 11:58:17.663634791   11687 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12657)\u001b[0m 2022-02-19 11:58:24,770\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12658)\u001b[0m 2022-02-19 11:58:25,476\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=12660)\u001b[0m 2022-02-19 11:58:25,665\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-19 11:58:26,699\tINFO trainable.py:125 -- Trainable.setup took 10.248 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-19 11:58:26,701\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 11:58:27,166\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 1\n",
      "Training iteration 2\n",
      "Training iteration 3\n",
      "Training iteration 4\n",
      "Training iteration 5\n",
      "Training iteration 6\n",
      "Training iteration 7\n",
      "Training iteration 8\n",
      "Training iteration 9\n",
      "Training iteration 10\n",
      "Training iteration 11\n",
      "Training iteration 12\n",
      "Training iteration 13\n",
      "Training iteration 14\n",
      "Training iteration 15\n",
      "Training iteration 16\n",
      "Training iteration 17\n",
      "Training iteration 18\n",
      "Training iteration 19\n"
     ]
    }
   ],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config[\"num_workers\"] = 3\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10\n",
    "\n",
    "trainer = PPOTrainer(trainer_config, ChainEnv)\n",
    "for i in range(20):\n",
    "    print(f\"Training iteration {i}\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80533975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1 20\n"
     ]
    }
   ],
   "source": [
    "env = ChainEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward = reward\n",
    "    \n",
    "print(cumulative_reward)\n",
    "print(max_state, env.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb27e",
   "metadata": {},
   "source": [
    "#### shaping reward\n",
    "\n",
    "위에서 마지막까지 가면 보상이 많아도, forward를 택하는 경우가 없음 -> 애초에 마지막까지 못가니까...?\n",
    "\n",
    "따라서 ShapedChainEnv.step을 수정하여 적절한 보상을 주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b57a3476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if behavior has been changed...\n",
      "Success! Behavior of environment is correct.\n"
     ]
    }
   ],
   "source": [
    "class ShapedChainEnv(ChainEnv):\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:\n",
    "            reward = -1\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:\n",
    "            reward = -1\n",
    "            self.state += 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "test_exercises.test_chain_env_behavior(ShapedChainEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(trainer_config, ShapedChainEnv)\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"training iteration: {i}\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7971a4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward you've received is: -498!\n",
      "Max state you've visited is: 4.2. This is out of 20 states.\n"
     ]
    }
   ],
   "source": [
    "env = ShapedChainEnv()\n",
    "max_states = []\n",
    "\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    max_state = -1\n",
    "    \n",
    "    while not done:\n",
    "        action = trainer.compute_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        max_state = max(max_state, state)\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "    max_states.append(max_state)\n",
    "\n",
    "print(\"Cumulative reward you've received is: {}!\".format(cumulative_reward))\n",
    "print(\"Max state you've visited is: {}. This is out of {} states.\".format(np.mean(max_states), env.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c02072",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Online learning with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbc486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcde55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e845ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a14452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc2884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
